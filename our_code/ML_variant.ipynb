{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This version of the script loads the preprocessed files, reserves one of each activity for testing and uses the others for training. \n",
    "\n",
    "### depending on setiup of feature_engi_second it may or may not be using the extra feature we calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training filenames: ['output_summary_cycling 2024-06-07 12-40-37.csv', 'output_summary_cycling 2024-06-08 12-45-52.csv', 'output_summary_running 2024-06-09 12-44-51.csv', 'output_summary_walking 2024-06-09 11-28-48.csv']\n",
      "Testing filenames: ['output_summary_cycling 2024-06-07 12-45-31.csv', 'output_summary_running 2024-06-09 12-49-21.csv', 'output_summary_walking 2024-06-09 11-42-12.csv']\n",
      "X_train shape: (13915, 26)\n",
      "y_train shape: (13915,)\n",
      "X_test shape: (7738, 26)\n",
      "y_test shape: (7738,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# Directory containing the files\n",
    "directory = '../preprocessed'\n",
    "\n",
    "# Initialize dictionaries to store filenames for each activity\n",
    "filenames = {\n",
    "    'cycling': [],\n",
    "    'running': [],\n",
    "    'walking': []\n",
    "}\n",
    "\n",
    "# Loop through the files and categorize them based on activity\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        if 'cycling' in filename:\n",
    "            filenames['cycling'].append(filename)\n",
    "        elif 'running' in filename:\n",
    "            filenames['running'].append(filename)\n",
    "        elif 'walking' in filename:\n",
    "            filenames['walking'].append(filename)\n",
    "\n",
    "# Function to randomly select one file for testing and use the rest for training\n",
    "def split_filenames(activity_filenames):\n",
    "    test_filename = random.choice(activity_filenames)\n",
    "    train_filenames = [f for f in activity_filenames if f != test_filename]\n",
    "    return train_filenames, test_filename\n",
    "\n",
    "# Split filenames for each activity\n",
    "train_cycling, test_cycling = split_filenames(filenames['cycling'])\n",
    "train_running, test_running = split_filenames(filenames['running'])\n",
    "train_walking, test_walking = split_filenames(filenames['walking'])\n",
    "\n",
    "# Combine all training filenames into a single list\n",
    "train_filenames = train_cycling + train_running + train_walking\n",
    "\n",
    "# Combine all testing filenames into a single list\n",
    "test_filenames = [test_cycling, test_running, test_walking]\n",
    "\n",
    "# Print the results\n",
    "print(\"Training filenames:\", train_filenames)\n",
    "print(\"Testing filenames:\", test_filenames)\n",
    "\n",
    "# Function to load data from filenames\n",
    "def load_data(filenames, directory):\n",
    "    data = []\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        data.append(df)\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Load training and testing data\n",
    "train_data = load_data(train_filenames, directory)\n",
    "test_data = load_data(test_filenames, directory)\n",
    "\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "\n",
    "# Assuming the label column is named 'activity'\n",
    "X_train = train_data.drop(columns=['date_time','activity'])\n",
    "y_train = train_data['activity']\n",
    "X_test = test_data.drop(columns=['date_time','activity'])\n",
    "y_test = test_data['activity']\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cycling       0.98      1.00      0.99      2343\n",
      "     running       0.98      0.99      0.99      1535\n",
      "     walking       1.00      0.98      0.99      3860\n",
      "\n",
      "    accuracy                           0.99      7738\n",
      "   macro avg       0.99      0.99      0.99      7738\n",
      "weighted avg       0.99      0.99      0.99      7738\n",
      "\n",
      "ROC AUC Score: 0.9982486107572289\n"
     ]
    }
   ],
   "source": [
    "bayes = GaussianNB()\n",
    "bayes.fit(X_train, y_train)\n",
    "pred_p = bayes.predict_proba(X_test)\n",
    "pred = bayes.predict(X_test)\n",
    "\n",
    "print('naive bayes')\n",
    "print(classification_report(y_pred=pred, y_true=y_test))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, pred_p, multi_class='ovr')\n",
    "print(f'ROC AUC Score: {roc_auc}')\n",
    "\n",
    "# RocCurveDisplay(*roc_curve(y_test, pred_p))\n",
    "# roc_curve(y_test, pred_p, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cycling       0.98      1.00      0.99      2343\n",
      "     running       0.99      0.98      0.99      1535\n",
      "     walking       1.00      0.99      0.99      3860\n",
      "\n",
      "    accuracy                           0.99      7738\n",
      "   macro avg       0.99      0.99      0.99      7738\n",
      "weighted avg       0.99      0.99      0.99      7738\n",
      "\n",
      "ROC AUC Score: 0.9983834838787051\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "pred_p = knn.predict_proba(X_test)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print('KNN')\n",
    "print(classification_report(y_pred=pred, y_true=y_test))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, pred_p, multi_class='ovr')\n",
    "print(f'ROC AUC Score: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN gridsearch\n",
      "{'leaf_size': 3, 'n_neighbors': 11}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cycling       0.98      1.00      0.99      2343\n",
      "     running       0.99      0.98      0.99      1535\n",
      "     walking       1.00      0.99      0.99      3860\n",
      "\n",
      "    accuracy                           0.99      7738\n",
      "   macro avg       0.99      0.99      0.99      7738\n",
      "weighted avg       0.99      0.99      0.99      7738\n",
      "\n",
      "ROC AUC Score: 0.9990684139172424\n"
     ]
    }
   ],
   "source": [
    "knn_gs = GridSearchCV(KNeighborsClassifier(), \n",
    "                      {\n",
    "                          'n_neighbors':[1, 3, 5, 11],\n",
    "                          'leaf_size':[3, 5, 10, 15]\n",
    "                        }, cv=5, scoring='roc_auc_ovr').fit(X_train, y_train)\n",
    "\n",
    "pred_p = knn_gs.predict_proba(X_test)\n",
    "pred = knn_gs.predict(X_test)\n",
    "\n",
    "print('KNN gridsearch')\n",
    "print(knn_gs.best_params_)\n",
    "print(classification_report(y_pred=pred, y_true=y_test))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, pred_p, multi_class='ovr')\n",
    "print(f'ROC AUC Score: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cycling       0.99      1.00      1.00      2343\n",
      "     running       0.97      0.99      0.98      1535\n",
      "     walking       1.00      0.99      0.99      3860\n",
      "\n",
      "    accuracy                           0.99      7738\n",
      "   macro avg       0.99      0.99      0.99      7738\n",
      "weighted avg       0.99      0.99      0.99      7738\n",
      "\n",
      "ROC AUC Score: 0.994822820717471\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "pred_p = tree.predict_proba(X_test)\n",
    "pred = tree.predict(X_test)\n",
    "\n",
    "print('decision tree')\n",
    "print(classification_report(y_pred=pred, y_true=y_test))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, pred_p, multi_class='ovr')\n",
    "print(f'ROC AUC Score: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree gridsearch\n",
      "{'max_depth': 5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cycling       0.98      1.00      0.99      2343\n",
      "     running       0.99      0.99      0.99      1535\n",
      "     walking       1.00      0.99      0.99      3860\n",
      "\n",
      "    accuracy                           0.99      7738\n",
      "   macro avg       0.99      0.99      0.99      7738\n",
      "weighted avg       0.99      0.99      0.99      7738\n",
      "\n",
      "ROC AUC Score: 0.994782082758834\n"
     ]
    }
   ],
   "source": [
    "tree_gs = GridSearchCV(DecisionTreeClassifier(), \n",
    "                      {\n",
    "                          'max_depth':[5, 10, 15, 50],\n",
    "                        }, cv=5, scoring='roc_auc_ovr').fit(X_train, y_train)\n",
    "\n",
    "pred_p = tree_gs.predict_proba(X_test)\n",
    "pred = tree_gs.predict(X_test)\n",
    "\n",
    "print('tree gridsearch')\n",
    "print(tree_gs.best_params_)\n",
    "print(classification_report(y_pred=pred, y_true=y_test))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, pred_p, multi_class='ovr')\n",
    "print(f'ROC AUC Score: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cycling       1.00      1.00      1.00      2343\n",
      "     running       0.98      0.99      0.99      1535\n",
      "     walking       1.00      0.99      1.00      3860\n",
      "\n",
      "    accuracy                           0.99      7738\n",
      "   macro avg       0.99      1.00      0.99      7738\n",
      "weighted avg       0.99      0.99      0.99      7738\n",
      "\n",
      "ROC AUC Score: 0.9999164561177833\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train, y_train)\n",
    "pred_p = forest.predict_proba(X_test)\n",
    "pred = forest.predict(X_test)\n",
    "\n",
    "print('Random forest')\n",
    "print(classification_report(y_pred=pred, y_true=y_test))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, pred_p, multi_class='ovr')\n",
    "print(f'ROC AUC Score: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest gridsearch\n",
      "{'max_depth': 5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     cycling       0.99      1.00      0.99      2343\n",
      "     running       0.98      0.99      0.98      1535\n",
      "     walking       1.00      0.99      0.99      3860\n",
      "\n",
      "    accuracy                           0.99      7738\n",
      "   macro avg       0.99      0.99      0.99      7738\n",
      "weighted avg       0.99      0.99      0.99      7738\n",
      "\n",
      "ROC AUC Score: 0.9998166919157033\n"
     ]
    }
   ],
   "source": [
    "forest_gs = GridSearchCV(RandomForestClassifier(), \n",
    "                      {\n",
    "                          'max_depth':[3, 5, 10, 20],\n",
    "                          'n_estimators':[10, 50, 100]\n",
    "                        }, cv=5, scoring='roc_auc_ovr').fit(X_train, y_train)\n",
    "\n",
    "pred_p = forest_gs.predict_proba(X_test)\n",
    "pred = forest_gs.predict(X_test)\n",
    "\n",
    "print('random forest gridsearch')\n",
    "print(tree_gs.best_params_)\n",
    "print(classification_report(y_pred=pred, y_true=y_test))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, pred_p, multi_class='ovr')\n",
    "print(f'ROC AUC Score: {roc_auc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4qs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
